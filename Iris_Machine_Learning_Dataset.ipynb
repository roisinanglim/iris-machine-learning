{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Package Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For building neural networks.\n",
    "import keras as kr\n",
    "\n",
    "# For interacting with data sets.\n",
    "import pandas as pd\n",
    "\n",
    "# For encoding categorical variables.\n",
    "import sklearn.preprocessing as pre\n",
    "\n",
    "# For splitting into training and test datasets.\n",
    "import sklearn.model_selection as mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inputs for neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal.lenght</th>\n",
       "      <th>sepal.width</th>\n",
       "      <th>petal.lenght</th>\n",
       "      <th>petal.width</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal.lenght  sepal.width  petal.lenght  petal.width   class\n",
       "0           5.1          3.5           1.4          0.2  Setosa\n",
       "1           4.9          3.0           1.4          0.2  Setosa\n",
       "2           4.7          3.2           1.3          0.2  Setosa\n",
       "3           4.6          3.1           1.5          0.2  Setosa\n",
       "4           5.0          3.6           1.4          0.2  Setosa"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the iris data set\n",
    "#Read in data from a csv file\n",
    "headings = [\"sepal.lenght\",\"sepal.width\",\"petal.lenght\",\"petal.width\",\"class\"]\n",
    "df = pd.read_csv('iris.csv',names = headings) \n",
    "#Print first five lines\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = df[[\"sepal.lenght\",\"sepal.width\",\"petal.lenght\",\"petal.width\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setosa→[1,0,0]\n",
    "versicolor→[0,1,0]\n",
    "virginica→[0,0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1]])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode the classes as above.\n",
    "encoder = pre.LabelBinarizer()\n",
    "encoder.fit(df[\"class\"])\n",
    "outputs = encoder.transform(df[\"class\"])\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea\n",
    "The neural network will turn four floating point inputs into three \"floating point\" outputs.\n",
    "\n",
    "[5.1,3.5,1.4,0.2]→[0.8,0.19,0.01]\n",
    "[5.1,3.5,1.4,0.2]→[1,0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a neural network, building it by layers.\n",
    "model = kr.models.Sequential()\n",
    "\n",
    "# Add a hidden layer with x neurons and an input layer with 4.\n",
    "model.add(kr.layers.Dense(units=30, activation='relu', input_dim=4))\n",
    "# Add a three neuron output layer.\n",
    "model.add(kr.layers.Dense(units=3, activation='softmax'))\n",
    "\n",
    "# Build the graph.\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_train, inputs_test, outputs_train, outputs_test = mod.train_test_split(inputs, outputs, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sepal.lenght    5.3\n",
       "sepal.width     3.7\n",
       "petal.lenght    1.5\n",
       "petal.width     0.2\n",
       "Name: 48, dtype: float64"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_test.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Harvey Norman\\Anaconda3\\New folder\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.355507  , 0.15013431, 0.49435872]], dtype=float32)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(inputs_test.as_matrix()[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 1.1266 - accuracy: 0.4333\n",
      "Epoch 2/15\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.7990 - accuracy: 0.9000\n",
      "Epoch 3/15\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.7081 - accuracy: 0.9167\n",
      "Epoch 4/15\n",
      "120/120 [==============================] - 0s 183us/step - loss: 0.6567 - accuracy: 0.9417\n",
      "Epoch 5/15\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.6176 - accuracy: 0.9083\n",
      "Epoch 6/15\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.5738 - accuracy: 0.9333\n",
      "Epoch 7/15\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.5403 - accuracy: 0.9500\n",
      "Epoch 8/15\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.5205 - accuracy: 0.9417\n",
      "Epoch 9/15\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.5003 - accuracy: 0.9417\n",
      "Epoch 10/15\n",
      "120/120 [==============================] - 0s 183us/step - loss: 0.4734 - accuracy: 0.9500\n",
      "Epoch 11/15\n",
      "120/120 [==============================] - 0s 191us/step - loss: 0.4592 - accuracy: 0.9250\n",
      "Epoch 12/15\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.4344 - accuracy: 0.9333\n",
      "Epoch 13/15\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.4258 - accuracy: 0.9167\n",
      "Epoch 14/15\n",
      "120/120 [==============================] - 0s 174us/step - loss: 0.4052 - accuracy: 0.9417\n",
      "Epoch 15/15\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.3946 - accuracy: 0.9417\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x284b60d7988>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the neural network.\n",
    "model.fit(inputs_train, outputs_train, epochs=15, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Harvey Norman\\Anaconda3\\New folder\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.8878418 , 0.08796514, 0.02419314]], dtype=float32)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(inputs_test.as_matrix()[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Setosa', 'Versicolor', 'Setosa', 'Virginica', 'Virginica',\n",
       "       'Setosa', 'Versicolor', 'Virginica', 'Setosa', 'Setosa', 'Setosa',\n",
       "       'Virginica', 'Virginica', 'Setosa', 'Setosa', 'Virginica',\n",
       "       'Virginica', 'Versicolor', 'Versicolor', 'Virginica', 'Virginica',\n",
       "       'Versicolor', 'Versicolor', 'Versicolor', 'Versicolor',\n",
       "       'Virginica', 'Virginica', 'Virginica', 'Virginica', 'Setosa'],\n",
       "      dtype='<U10')"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Have the network predict the classes of the test inputs.\n",
    "predictions = model.predict(inputs_test)\n",
    "predictions_labels = encoder.inverse_transform(predictions)\n",
    "predictions_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare the predictions to the actual classes.\n",
    "predictions_labels == encoder.inverse_transform(outputs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(predictions_labels == encoder.inverse_transform(outputs_test)).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Whitening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.decomposition as dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal.lenght</th>\n",
       "      <th>sepal.width</th>\n",
       "      <th>petal.lenght</th>\n",
       "      <th>petal.width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.311283</td>\n",
       "      <td>0.544051</td>\n",
       "      <td>0.319564</td>\n",
       "      <td>0.475325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.989670</td>\n",
       "      <td>0.367736</td>\n",
       "      <td>1.656846</td>\n",
       "      <td>-1.262023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.487690</td>\n",
       "      <td>0.695242</td>\n",
       "      <td>-1.148338</td>\n",
       "      <td>-0.599249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.170631</td>\n",
       "      <td>0.811306</td>\n",
       "      <td>0.926588</td>\n",
       "      <td>0.546250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.192257</td>\n",
       "      <td>-2.657387</td>\n",
       "      <td>-0.783509</td>\n",
       "      <td>-0.345680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.261006</td>\n",
       "      <td>-0.389753</td>\n",
       "      <td>-0.440380</td>\n",
       "      <td>-0.106823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1.289260</td>\n",
       "      <td>0.524290</td>\n",
       "      <td>-0.088639</td>\n",
       "      <td>0.045132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.600904</td>\n",
       "      <td>-0.097389</td>\n",
       "      <td>1.599839</td>\n",
       "      <td>0.630841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1.220464</td>\n",
       "      <td>0.782025</td>\n",
       "      <td>0.726940</td>\n",
       "      <td>-0.164449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1.105076</td>\n",
       "      <td>0.675652</td>\n",
       "      <td>-0.845098</td>\n",
       "      <td>0.134739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.450797</td>\n",
       "      <td>-0.301942</td>\n",
       "      <td>0.131450</td>\n",
       "      <td>0.266647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.723184</td>\n",
       "      <td>1.265392</td>\n",
       "      <td>-1.342788</td>\n",
       "      <td>0.815071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-1.385441</td>\n",
       "      <td>-0.439472</td>\n",
       "      <td>0.130136</td>\n",
       "      <td>-0.092940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.504314</td>\n",
       "      <td>0.693983</td>\n",
       "      <td>-0.110956</td>\n",
       "      <td>0.091897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.692464</td>\n",
       "      <td>-0.643444</td>\n",
       "      <td>-1.197266</td>\n",
       "      <td>0.006494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-1.205223</td>\n",
       "      <td>1.200679</td>\n",
       "      <td>-0.296471</td>\n",
       "      <td>0.171677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.165825</td>\n",
       "      <td>-0.704947</td>\n",
       "      <td>-0.927907</td>\n",
       "      <td>1.402613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.734669</td>\n",
       "      <td>-0.810672</td>\n",
       "      <td>0.073320</td>\n",
       "      <td>1.219795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.117203</td>\n",
       "      <td>0.548736</td>\n",
       "      <td>0.073864</td>\n",
       "      <td>-0.951307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.487601</td>\n",
       "      <td>-0.038114</td>\n",
       "      <td>-0.053208</td>\n",
       "      <td>1.073058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.170855</td>\n",
       "      <td>-0.537712</td>\n",
       "      <td>0.679163</td>\n",
       "      <td>1.037675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.674140</td>\n",
       "      <td>1.441111</td>\n",
       "      <td>-1.518481</td>\n",
       "      <td>-0.056393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-1.277152</td>\n",
       "      <td>1.555953</td>\n",
       "      <td>0.719851</td>\n",
       "      <td>1.955846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-1.255500</td>\n",
       "      <td>2.134483</td>\n",
       "      <td>0.463473</td>\n",
       "      <td>0.753825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.008134</td>\n",
       "      <td>0.494183</td>\n",
       "      <td>0.607235</td>\n",
       "      <td>-2.644564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.754516</td>\n",
       "      <td>-1.104137</td>\n",
       "      <td>1.084501</td>\n",
       "      <td>0.192275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-1.054744</td>\n",
       "      <td>0.806940</td>\n",
       "      <td>1.020092</td>\n",
       "      <td>1.307699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.182024</td>\n",
       "      <td>-1.173007</td>\n",
       "      <td>-2.363833</td>\n",
       "      <td>-0.433177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.562953</td>\n",
       "      <td>1.542777</td>\n",
       "      <td>-1.275826</td>\n",
       "      <td>-1.922495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-1.055577</td>\n",
       "      <td>1.682720</td>\n",
       "      <td>-0.481956</td>\n",
       "      <td>-0.091879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>-1.298731</td>\n",
       "      <td>-0.511157</td>\n",
       "      <td>-0.662640</td>\n",
       "      <td>-0.640505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.919953</td>\n",
       "      <td>0.252425</td>\n",
       "      <td>0.434349</td>\n",
       "      <td>-0.779713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.019403</td>\n",
       "      <td>-1.479195</td>\n",
       "      <td>-0.768622</td>\n",
       "      <td>-0.052497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.765330</td>\n",
       "      <td>1.080795</td>\n",
       "      <td>-1.262931</td>\n",
       "      <td>0.075661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.994698</td>\n",
       "      <td>0.947526</td>\n",
       "      <td>0.363086</td>\n",
       "      <td>-3.155191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>-1.296465</td>\n",
       "      <td>0.083350</td>\n",
       "      <td>-0.276325</td>\n",
       "      <td>-0.187256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1.774806</td>\n",
       "      <td>1.068519</td>\n",
       "      <td>-2.123334</td>\n",
       "      <td>0.958901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.585625</td>\n",
       "      <td>0.640800</td>\n",
       "      <td>0.541853</td>\n",
       "      <td>0.665672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>-1.297810</td>\n",
       "      <td>-0.653581</td>\n",
       "      <td>-0.255722</td>\n",
       "      <td>-0.913234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1.334968</td>\n",
       "      <td>1.256134</td>\n",
       "      <td>-0.841051</td>\n",
       "      <td>1.663377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.363625</td>\n",
       "      <td>0.047232</td>\n",
       "      <td>-0.729648</td>\n",
       "      <td>0.186461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>-1.250242</td>\n",
       "      <td>-0.095021</td>\n",
       "      <td>0.388714</td>\n",
       "      <td>1.077913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>-1.097373</td>\n",
       "      <td>0.094813</td>\n",
       "      <td>0.177027</td>\n",
       "      <td>-0.904309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>1.011938</td>\n",
       "      <td>0.173172</td>\n",
       "      <td>0.110552</td>\n",
       "      <td>1.122876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0.807148</td>\n",
       "      <td>-0.710078</td>\n",
       "      <td>-0.384258</td>\n",
       "      <td>-1.608932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>-1.279010</td>\n",
       "      <td>-0.381135</td>\n",
       "      <td>-0.637724</td>\n",
       "      <td>0.375239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>-1.346371</td>\n",
       "      <td>0.418863</td>\n",
       "      <td>0.331793</td>\n",
       "      <td>1.166720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>1.191859</td>\n",
       "      <td>0.510481</td>\n",
       "      <td>1.097732</td>\n",
       "      <td>-1.654478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0.722651</td>\n",
       "      <td>-1.521532</td>\n",
       "      <td>1.070529</td>\n",
       "      <td>-0.834997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>-1.257447</td>\n",
       "      <td>-0.535961</td>\n",
       "      <td>0.201028</td>\n",
       "      <td>0.845525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>-1.093762</td>\n",
       "      <td>1.426251</td>\n",
       "      <td>0.542739</td>\n",
       "      <td>0.274918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>-1.154396</td>\n",
       "      <td>0.722742</td>\n",
       "      <td>-0.497670</td>\n",
       "      <td>-1.455028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0.164109</td>\n",
       "      <td>-0.843587</td>\n",
       "      <td>-0.755478</td>\n",
       "      <td>-0.095193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>1.101411</td>\n",
       "      <td>-0.325785</td>\n",
       "      <td>0.538531</td>\n",
       "      <td>-0.242876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0.444106</td>\n",
       "      <td>0.457275</td>\n",
       "      <td>1.321943</td>\n",
       "      <td>0.888887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0.197817</td>\n",
       "      <td>-0.711040</td>\n",
       "      <td>0.072330</td>\n",
       "      <td>0.386621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0.736600</td>\n",
       "      <td>-0.362163</td>\n",
       "      <td>-0.207974</td>\n",
       "      <td>-0.997971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>-1.258740</td>\n",
       "      <td>0.204754</td>\n",
       "      <td>-0.054064</td>\n",
       "      <td>0.346426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>-1.218748</td>\n",
       "      <td>0.920665</td>\n",
       "      <td>0.554511</td>\n",
       "      <td>1.333357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>-1.229512</td>\n",
       "      <td>0.625933</td>\n",
       "      <td>-0.274582</td>\n",
       "      <td>0.148621</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal.lenght  sepal.width  petal.lenght  petal.width\n",
       "0       -1.311283     0.544051      0.319564     0.475325\n",
       "1        0.989670     0.367736      1.656846    -1.262023\n",
       "2        0.487690     0.695242     -1.148338    -0.599249\n",
       "3        1.170631     0.811306      0.926588     0.546250\n",
       "4       -0.192257    -2.657387     -0.783509    -0.345680\n",
       "5       -1.261006    -0.389753     -0.440380    -0.106823\n",
       "6       -1.289260     0.524290     -0.088639     0.045132\n",
       "7        0.600904    -0.097389      1.599839     0.630841\n",
       "8       -1.220464     0.782025      0.726940    -0.164449\n",
       "9       -1.105076     0.675652     -0.845098     0.134739\n",
       "10       0.450797    -0.301942      0.131450     0.266647\n",
       "11       1.723184     1.265392     -1.342788     0.815071\n",
       "12      -1.385441    -0.439472      0.130136    -0.092940\n",
       "13       0.504314     0.693983     -0.110956     0.091897\n",
       "14       0.692464    -0.643444     -1.197266     0.006494\n",
       "15      -1.205223     1.200679     -0.296471     0.171677\n",
       "16       0.165825    -0.704947     -0.927907     1.402613\n",
       "17       0.734669    -0.810672      0.073320     1.219795\n",
       "18       1.117203     0.548736      0.073864    -0.951307\n",
       "19       0.487601    -0.038114     -0.053208     1.073058\n",
       "20       0.170855    -0.537712      0.679163     1.037675\n",
       "21       0.674140     1.441111     -1.518481    -0.056393\n",
       "22      -1.277152     1.555953      0.719851     1.955846\n",
       "23      -1.255500     2.134483      0.463473     0.753825\n",
       "24       1.008134     0.494183      0.607235    -2.644564\n",
       "25       0.754516    -1.104137      1.084501     0.192275\n",
       "26      -1.054744     0.806940      1.020092     1.307699\n",
       "27       0.182024    -1.173007     -2.363833    -0.433177\n",
       "28       1.562953     1.542777     -1.275826    -1.922495\n",
       "29      -1.055577     1.682720     -0.481956    -0.091879\n",
       "..            ...          ...           ...          ...\n",
       "90      -1.298731    -0.511157     -0.662640    -0.640505\n",
       "91       0.919953     0.252425      0.434349    -0.779713\n",
       "92       0.019403    -1.479195     -0.768622    -0.052497\n",
       "93       0.765330     1.080795     -1.262931     0.075661\n",
       "94       0.994698     0.947526      0.363086    -3.155191\n",
       "95      -1.296465     0.083350     -0.276325    -0.187256\n",
       "96       1.774806     1.068519     -2.123334     0.958901\n",
       "97       0.585625     0.640800      0.541853     0.665672\n",
       "98      -1.297810    -0.653581     -0.255722    -0.913234\n",
       "99       1.334968     1.256134     -0.841051     1.663377\n",
       "100      0.363625     0.047232     -0.729648     0.186461\n",
       "101     -1.250242    -0.095021      0.388714     1.077913\n",
       "102     -1.097373     0.094813      0.177027    -0.904309\n",
       "103      1.011938     0.173172      0.110552     1.122876\n",
       "104      0.807148    -0.710078     -0.384258    -1.608932\n",
       "105     -1.279010    -0.381135     -0.637724     0.375239\n",
       "106     -1.346371     0.418863      0.331793     1.166720\n",
       "107      1.191859     0.510481      1.097732    -1.654478\n",
       "108      0.722651    -1.521532      1.070529    -0.834997\n",
       "109     -1.257447    -0.535961      0.201028     0.845525\n",
       "110     -1.093762     1.426251      0.542739     0.274918\n",
       "111     -1.154396     0.722742     -0.497670    -1.455028\n",
       "112      0.164109    -0.843587     -0.755478    -0.095193\n",
       "113      1.101411    -0.325785      0.538531    -0.242876\n",
       "114      0.444106     0.457275      1.321943     0.888887\n",
       "115      0.197817    -0.711040      0.072330     0.386621\n",
       "116      0.736600    -0.362163     -0.207974    -0.997971\n",
       "117     -1.258740     0.204754     -0.054064     0.346426\n",
       "118     -1.218748     0.920665      0.554511     1.333357\n",
       "119     -1.229512     0.625933     -0.274582     0.148621\n",
       "\n",
       "[120 rows x 4 columns]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = dec.PCA(n_components=4, whiten=True)\n",
    "pca.fit(inputs_train)\n",
    "inputs_train_white = pd.DataFrame(pca.transform(inputs_train), columns=inputs_train.columns)\n",
    "inputs_train_white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a neural network, building it by layers.\n",
    "model = kr.models.Sequential()\n",
    "\n",
    "# Add a hidden layer with x neurons and an input layer with 4.\n",
    "model.add(kr.layers.Dense(units=30, activation='relu', input_dim=4))\n",
    "# Add a three neuron output layer.\n",
    "model.add(kr.layers.Dense(units=3, activation='softmax'))\n",
    "\n",
    "# Build the graph.\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 1.3805 - accuracy: 0.0833\n",
      "Epoch 2/15\n",
      "120/120 [==============================] - 0s 166us/step - loss: 1.2933 - accuracy: 0.1167\n",
      "Epoch 3/15\n",
      "120/120 [==============================] - 0s 166us/step - loss: 1.2184 - accuracy: 0.2000\n",
      "Epoch 4/15\n",
      "120/120 [==============================] - 0s 141us/step - loss: 1.1514 - accuracy: 0.3333\n",
      "Epoch 5/15\n",
      "120/120 [==============================] - 0s 166us/step - loss: 1.0915 - accuracy: 0.4583\n",
      "Epoch 6/15\n",
      "120/120 [==============================] - 0s 150us/step - loss: 1.0387 - accuracy: 0.5333\n",
      "Epoch 7/15\n",
      "120/120 [==============================] - 0s 175us/step - loss: 0.9885 - accuracy: 0.5917\n",
      "Epoch 8/15\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.9434 - accuracy: 0.6500\n",
      "Epoch 9/15\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.9022 - accuracy: 0.6667\n",
      "Epoch 10/15\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.8634 - accuracy: 0.6917\n",
      "Epoch 11/15\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.8280 - accuracy: 0.7083\n",
      "Epoch 12/15\n",
      "120/120 [==============================] - 0s 175us/step - loss: 0.7946 - accuracy: 0.7250\n",
      "Epoch 13/15\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.7647 - accuracy: 0.7583\n",
      "Epoch 14/15\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.7369 - accuracy: 0.7750\n",
      "Epoch 15/15\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.7109 - accuracy: 0.8167\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x284b604dec8>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the neural network.\n",
    "model.fit(inputs_train_white, outputs_train, epochs=15, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Setosa', 'Versicolor', 'Setosa', 'Virginica', 'Virginica',\n",
       "       'Setosa', 'Virginica', 'Virginica', 'Setosa', 'Setosa', 'Setosa',\n",
       "       'Versicolor', 'Virginica', 'Setosa', 'Setosa', 'Virginica',\n",
       "       'Versicolor', 'Virginica', 'Versicolor', 'Virginica', 'Virginica',\n",
       "       'Virginica', 'Virginica', 'Versicolor', 'Virginica', 'Virginica',\n",
       "       'Virginica', 'Versicolor', 'Virginica', 'Setosa'], dtype='<U10')"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Have the network predict the classes of the test inputs.\n",
    "predictions = model.predict(pca.transform(inputs_test))\n",
    "predictions_labels = encoder.inverse_transform(predictions)\n",
    "predictions_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(predictions_labels == encoder.inverse_transform(outputs_test)).sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
