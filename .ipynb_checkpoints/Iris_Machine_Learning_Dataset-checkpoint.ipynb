{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Package Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For building neural networks.\n",
    "import keras as kr\n",
    "\n",
    "# For interacting with data sets.\n",
    "import pandas as pd\n",
    "\n",
    "# For encoding categorical variables.\n",
    "import sklearn.preprocessing as pre\n",
    "\n",
    "# For splitting into training and test datasets.\n",
    "import sklearn.model_selection as mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inputs for neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal.lenght</th>\n",
       "      <th>sepal.width</th>\n",
       "      <th>petal.lenght</th>\n",
       "      <th>petal.width</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal.lenght  sepal.width  petal.lenght  petal.width   class\n",
       "0           5.1          3.5           1.4          0.2  Setosa\n",
       "1           4.9          3.0           1.4          0.2  Setosa\n",
       "2           4.7          3.2           1.3          0.2  Setosa\n",
       "3           4.6          3.1           1.5          0.2  Setosa\n",
       "4           5.0          3.6           1.4          0.2  Setosa"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the iris data set\n",
    "#Read in data from a csv file\n",
    "headings = [\"sepal.lenght\",\"sepal.width\",\"petal.lenght\",\"petal.width\",\"class\"]\n",
    "df = pd.read_csv('iris.csv',names = headings) \n",
    "#Print first five lines\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = df[[\"sepal.lenght\",\"sepal.width\",\"petal.lenght\",\"petal.width\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setosa→[1,0,0]\n",
    "versicolor→[0,1,0]\n",
    "virginica→[0,0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode the classes as above.\n",
    "encoder = pre.LabelBinarizer()\n",
    "encoder.fit(df[\"class\"])\n",
    "outputs = encoder.transform(df[\"class\"])\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea\n",
    "The neural network will turn four floating point inputs into three \"floating point\" outputs.\n",
    "\n",
    "[5.1,3.5,1.4,0.2]→[0.8,0.19,0.01]\n",
    "[5.1,3.5,1.4,0.2]→[1,0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a neural network, building it by layers.\n",
    "model = kr.models.Sequential()\n",
    "\n",
    "# Add a hidden layer with x neurons and an input layer with 4.\n",
    "model.add(kr.layers.Dense(units=30, activation='relu', input_dim=4))\n",
    "# Add a three neuron output layer.\n",
    "model.add(kr.layers.Dense(units=3, activation='softmax'))\n",
    "\n",
    "# Build the graph.\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_train, inputs_test, outputs_train, outputs_test = mod.train_test_split(inputs, outputs, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sepal.lenght    5.1\n",
       "sepal.width     3.8\n",
       "petal.lenght    1.9\n",
       "petal.width     0.4\n",
       "Name: 44, dtype: float64"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_test.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Harvey Norman\\Anaconda3\\New folder\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.05287564, 0.17231598, 0.77480835]], dtype=float32)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(inputs_test.as_matrix()[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 1.1564 - accuracy: 0.3833\n",
      "Epoch 2/15\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.9675 - accuracy: 0.5917\n",
      "Epoch 3/15\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.8384 - accuracy: 0.7083\n",
      "Epoch 4/15\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.7384 - accuracy: 0.7333\n",
      "Epoch 5/15\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.6555 - accuracy: 0.8333\n",
      "Epoch 6/15\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.6111 - accuracy: 0.8250\n",
      "Epoch 7/15\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.5665 - accuracy: 0.8083\n",
      "Epoch 8/15\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.5335 - accuracy: 0.8500\n",
      "Epoch 9/15\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.5114 - accuracy: 0.9167\n",
      "Epoch 10/15\n",
      "120/120 [==============================] - 0s 183us/step - loss: 0.4994 - accuracy: 0.8750\n",
      "Epoch 11/15\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.4715 - accuracy: 0.9083\n",
      "Epoch 12/15\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.4686 - accuracy: 0.8583\n",
      "Epoch 13/15\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.4355 - accuracy: 0.9333\n",
      "Epoch 14/15\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.4439 - accuracy: 0.8583\n",
      "Epoch 15/15\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.4214 - accuracy: 0.8833\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x284b3c8fc88>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the neural network.\n",
    "model.fit(inputs_train, outputs_train, epochs=15, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Harvey Norman\\Anaconda3\\New folder\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.8229614 , 0.15686432, 0.02017432]], dtype=float32)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(inputs_test.as_matrix()[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Setosa', 'Virginica', 'Virginica', 'Versicolor', 'Versicolor',\n",
       "       'Setosa', 'Virginica', 'Setosa', 'Versicolor', 'Setosa',\n",
       "       'Versicolor', 'Versicolor', 'Virginica', 'Versicolor', 'Virginica',\n",
       "       'Setosa', 'Virginica', 'Virginica', 'Virginica', 'Versicolor',\n",
       "       'Versicolor', 'Setosa', 'Versicolor', 'Virginica', 'Setosa',\n",
       "       'Setosa', 'Versicolor', 'Setosa', 'Setosa', 'Versicolor'],\n",
       "      dtype='<U10')"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Have the network predict the classes of the test inputs.\n",
    "predictions = model.predict(inputs_test)\n",
    "predictions_labels = encoder.inverse_transform(predictions)\n",
    "predictions_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare the predictions to the actual classes.\n",
    "predictions_labels == encoder.inverse_transform(outputs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(predictions_labels == encoder.inverse_transform(outputs_test)).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Whitening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.decomposition as dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal.lenght</th>\n",
       "      <th>sepal.width</th>\n",
       "      <th>petal.lenght</th>\n",
       "      <th>petal.width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.108120</td>\n",
       "      <td>0.302344</td>\n",
       "      <td>1.076124</td>\n",
       "      <td>-1.573096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.452585</td>\n",
       "      <td>-1.510879</td>\n",
       "      <td>0.031232</td>\n",
       "      <td>-1.354474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.283049</td>\n",
       "      <td>-0.350376</td>\n",
       "      <td>0.171078</td>\n",
       "      <td>0.840641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.919787</td>\n",
       "      <td>0.746865</td>\n",
       "      <td>0.373252</td>\n",
       "      <td>-3.090540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.669750</td>\n",
       "      <td>-1.182844</td>\n",
       "      <td>1.011514</td>\n",
       "      <td>0.318330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.260795</td>\n",
       "      <td>0.489916</td>\n",
       "      <td>-0.262416</td>\n",
       "      <td>0.080378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1.148868</td>\n",
       "      <td>-0.031179</td>\n",
       "      <td>0.424478</td>\n",
       "      <td>1.970262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1.383144</td>\n",
       "      <td>-0.407547</td>\n",
       "      <td>0.335892</td>\n",
       "      <td>0.442062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.048639</td>\n",
       "      <td>-1.430685</td>\n",
       "      <td>-0.827510</td>\n",
       "      <td>0.028601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.678043</td>\n",
       "      <td>2.259234</td>\n",
       "      <td>0.251093</td>\n",
       "      <td>2.039230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.753844</td>\n",
       "      <td>-1.112128</td>\n",
       "      <td>2.211184</td>\n",
       "      <td>-1.782719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.611540</td>\n",
       "      <td>-1.573949</td>\n",
       "      <td>-1.211912</td>\n",
       "      <td>0.366307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.658037</td>\n",
       "      <td>-0.461935</td>\n",
       "      <td>-0.243536</td>\n",
       "      <td>-0.929063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.440374</td>\n",
       "      <td>0.594110</td>\n",
       "      <td>-0.126508</td>\n",
       "      <td>0.061718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.168780</td>\n",
       "      <td>-0.604994</td>\n",
       "      <td>0.240359</td>\n",
       "      <td>0.957044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.404524</td>\n",
       "      <td>0.616511</td>\n",
       "      <td>-1.608180</td>\n",
       "      <td>1.627832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-1.406713</td>\n",
       "      <td>-0.245502</td>\n",
       "      <td>0.113342</td>\n",
       "      <td>-0.085045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-1.323052</td>\n",
       "      <td>-0.323268</td>\n",
       "      <td>-0.672991</td>\n",
       "      <td>-0.636055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.018480</td>\n",
       "      <td>-1.448096</td>\n",
       "      <td>1.000214</td>\n",
       "      <td>0.239146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-1.562297</td>\n",
       "      <td>0.326252</td>\n",
       "      <td>1.056850</td>\n",
       "      <td>0.052775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.065301</td>\n",
       "      <td>-1.383220</td>\n",
       "      <td>-0.189461</td>\n",
       "      <td>-0.067950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-1.246709</td>\n",
       "      <td>0.765495</td>\n",
       "      <td>-0.266236</td>\n",
       "      <td>0.073073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-1.122189</td>\n",
       "      <td>0.224842</td>\n",
       "      <td>0.176701</td>\n",
       "      <td>-0.901112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-1.236191</td>\n",
       "      <td>1.189566</td>\n",
       "      <td>-0.070571</td>\n",
       "      <td>0.277008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.107392</td>\n",
       "      <td>-1.380172</td>\n",
       "      <td>-1.046792</td>\n",
       "      <td>0.182496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.841369</td>\n",
       "      <td>0.101731</td>\n",
       "      <td>0.406119</td>\n",
       "      <td>-0.731663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.097159</td>\n",
       "      <td>-0.837910</td>\n",
       "      <td>-0.799927</td>\n",
       "      <td>-0.049889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.553158</td>\n",
       "      <td>-0.362764</td>\n",
       "      <td>0.945587</td>\n",
       "      <td>0.051732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.402185</td>\n",
       "      <td>1.489895</td>\n",
       "      <td>1.353287</td>\n",
       "      <td>-0.529269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-1.285644</td>\n",
       "      <td>1.672001</td>\n",
       "      <td>0.730635</td>\n",
       "      <td>1.802972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>-1.259942</td>\n",
       "      <td>2.221158</td>\n",
       "      <td>0.500999</td>\n",
       "      <td>0.587557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>-1.392382</td>\n",
       "      <td>-1.851658</td>\n",
       "      <td>-1.075374</td>\n",
       "      <td>-2.011575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.513515</td>\n",
       "      <td>0.092272</td>\n",
       "      <td>-1.108506</td>\n",
       "      <td>-0.693227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>1.554620</td>\n",
       "      <td>2.649393</td>\n",
       "      <td>-0.613325</td>\n",
       "      <td>1.601369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>1.142134</td>\n",
       "      <td>0.843816</td>\n",
       "      <td>-1.701690</td>\n",
       "      <td>1.439723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.635542</td>\n",
       "      <td>-1.587512</td>\n",
       "      <td>0.996641</td>\n",
       "      <td>-0.656625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1.228337</td>\n",
       "      <td>0.488605</td>\n",
       "      <td>0.930002</td>\n",
       "      <td>-0.192257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.380465</td>\n",
       "      <td>-0.359716</td>\n",
       "      <td>0.088255</td>\n",
       "      <td>0.302178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.910400</td>\n",
       "      <td>0.190003</td>\n",
       "      <td>1.627749</td>\n",
       "      <td>-1.169448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.579340</td>\n",
       "      <td>0.756964</td>\n",
       "      <td>-0.897416</td>\n",
       "      <td>-0.188035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.209086</td>\n",
       "      <td>-1.359005</td>\n",
       "      <td>-0.113033</td>\n",
       "      <td>1.839996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.296010</td>\n",
       "      <td>-0.858359</td>\n",
       "      <td>0.098610</td>\n",
       "      <td>1.671490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.160514</td>\n",
       "      <td>-0.175219</td>\n",
       "      <td>-0.810833</td>\n",
       "      <td>-0.745502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>1.476785</td>\n",
       "      <td>1.265492</td>\n",
       "      <td>-1.260217</td>\n",
       "      <td>-1.964522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>1.123760</td>\n",
       "      <td>-0.139128</td>\n",
       "      <td>0.921663</td>\n",
       "      <td>0.194794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>1.089611</td>\n",
       "      <td>0.611036</td>\n",
       "      <td>0.894227</td>\n",
       "      <td>0.545320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>-1.407748</td>\n",
       "      <td>-1.107198</td>\n",
       "      <td>0.144047</td>\n",
       "      <td>0.245432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>-1.361987</td>\n",
       "      <td>0.583238</td>\n",
       "      <td>0.323880</td>\n",
       "      <td>1.093845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>-1.222958</td>\n",
       "      <td>-0.266097</td>\n",
       "      <td>-0.837805</td>\n",
       "      <td>-0.237476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>-1.282293</td>\n",
       "      <td>2.375353</td>\n",
       "      <td>-0.546374</td>\n",
       "      <td>-1.107684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>0.380162</td>\n",
       "      <td>0.367188</td>\n",
       "      <td>1.285716</td>\n",
       "      <td>0.895922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>1.678649</td>\n",
       "      <td>0.805426</td>\n",
       "      <td>-2.143828</td>\n",
       "      <td>0.851743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0.631383</td>\n",
       "      <td>0.419173</td>\n",
       "      <td>-1.879967</td>\n",
       "      <td>-0.269281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>-1.285398</td>\n",
       "      <td>-0.209735</td>\n",
       "      <td>-0.454261</td>\n",
       "      <td>-0.116252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>-1.305583</td>\n",
       "      <td>0.673366</td>\n",
       "      <td>-0.082176</td>\n",
       "      <td>-0.016944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>-1.257531</td>\n",
       "      <td>1.068328</td>\n",
       "      <td>0.735560</td>\n",
       "      <td>0.462881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0.669750</td>\n",
       "      <td>-1.182844</td>\n",
       "      <td>1.011514</td>\n",
       "      <td>0.318330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0.158261</td>\n",
       "      <td>-1.029065</td>\n",
       "      <td>0.044694</td>\n",
       "      <td>0.753109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0.492536</td>\n",
       "      <td>0.398962</td>\n",
       "      <td>-1.512401</td>\n",
       "      <td>0.224384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>-1.452536</td>\n",
       "      <td>-0.923748</td>\n",
       "      <td>0.324286</td>\n",
       "      <td>0.148110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal.lenght  sepal.width  petal.lenght  petal.width\n",
       "0        1.108120     0.302344      1.076124    -1.573096\n",
       "1       -0.452585    -1.510879      0.031232    -1.354474\n",
       "2       -1.283049    -0.350376      0.171078     0.840641\n",
       "3        0.919787     0.746865      0.373252    -3.090540\n",
       "4        0.669750    -1.182844      1.011514     0.318330\n",
       "5       -1.260795     0.489916     -0.262416     0.080378\n",
       "6       -1.148868    -0.031179      0.424478     1.970262\n",
       "7       -1.383144    -0.407547      0.335892     0.442062\n",
       "8       -0.048639    -1.430685     -0.827510     0.028601\n",
       "9        1.678043     2.259234      0.251093     2.039230\n",
       "10       0.753844    -1.112128      2.211184    -1.782719\n",
       "11       0.611540    -1.573949     -1.211912     0.366307\n",
       "12       0.658037    -0.461935     -0.243536    -0.929063\n",
       "13       0.440374     0.594110     -0.126508     0.061718\n",
       "14       0.168780    -0.604994      0.240359     0.957044\n",
       "15       1.404524     0.616511     -1.608180     1.627832\n",
       "16      -1.406713    -0.245502      0.113342    -0.085045\n",
       "17      -1.323052    -0.323268     -0.672991    -0.636055\n",
       "18      -0.018480    -1.448096      1.000214     0.239146\n",
       "19      -1.562297     0.326252      1.056850     0.052775\n",
       "20       0.065301    -1.383220     -0.189461    -0.067950\n",
       "21      -1.246709     0.765495     -0.266236     0.073073\n",
       "22      -1.122189     0.224842      0.176701    -0.901112\n",
       "23      -1.236191     1.189566     -0.070571     0.277008\n",
       "24      -0.107392    -1.380172     -1.046792     0.182496\n",
       "25       0.841369     0.101731      0.406119    -0.731663\n",
       "26       0.097159    -0.837910     -0.799927    -0.049889\n",
       "27       0.553158    -0.362764      0.945587     0.051732\n",
       "28       1.402185     1.489895      1.353287    -0.529269\n",
       "29      -1.285644     1.672001      0.730635     1.802972\n",
       "..            ...          ...           ...          ...\n",
       "90      -1.259942     2.221158      0.500999     0.587557\n",
       "91      -1.392382    -1.851658     -1.075374    -2.011575\n",
       "92       0.513515     0.092272     -1.108506    -0.693227\n",
       "93       1.554620     2.649393     -0.613325     1.601369\n",
       "94       1.142134     0.843816     -1.701690     1.439723\n",
       "95       0.635542    -1.587512      0.996641    -0.656625\n",
       "96       1.228337     0.488605      0.930002    -0.192257\n",
       "97       0.380465    -0.359716      0.088255     0.302178\n",
       "98       0.910400     0.190003      1.627749    -1.169448\n",
       "99       0.579340     0.756964     -0.897416    -0.188035\n",
       "100      0.209086    -1.359005     -0.113033     1.839996\n",
       "101      0.296010    -0.858359      0.098610     1.671490\n",
       "102      0.160514    -0.175219     -0.810833    -0.745502\n",
       "103      1.476785     1.265492     -1.260217    -1.964522\n",
       "104      1.123760    -0.139128      0.921663     0.194794\n",
       "105      1.089611     0.611036      0.894227     0.545320\n",
       "106     -1.407748    -1.107198      0.144047     0.245432\n",
       "107     -1.361987     0.583238      0.323880     1.093845\n",
       "108     -1.222958    -0.266097     -0.837805    -0.237476\n",
       "109     -1.282293     2.375353     -0.546374    -1.107684\n",
       "110      0.380162     0.367188      1.285716     0.895922\n",
       "111      1.678649     0.805426     -2.143828     0.851743\n",
       "112      0.631383     0.419173     -1.879967    -0.269281\n",
       "113     -1.285398    -0.209735     -0.454261    -0.116252\n",
       "114     -1.305583     0.673366     -0.082176    -0.016944\n",
       "115     -1.257531     1.068328      0.735560     0.462881\n",
       "116      0.669750    -1.182844      1.011514     0.318330\n",
       "117      0.158261    -1.029065      0.044694     0.753109\n",
       "118      0.492536     0.398962     -1.512401     0.224384\n",
       "119     -1.452536    -0.923748      0.324286     0.148110\n",
       "\n",
       "[120 rows x 4 columns]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = dec.PCA(n_components=4, whiten=True)\n",
    "pca.fit(inputs_train)\n",
    "inputs_train_white = pd.DataFrame(pca.transform(inputs_train), columns=inputs_train.columns)\n",
    "inputs_train_white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a neural network, building it by layers.\n",
    "model = kr.models.Sequential()\n",
    "\n",
    "# Add a hidden layer with x neurons and an input layer with 4.\n",
    "model.add(kr.layers.Dense(units=30, activation='relu', input_dim=4))\n",
    "# Add a three neuron output layer.\n",
    "model.add(kr.layers.Dense(units=3, activation='softmax'))\n",
    "\n",
    "# Build the graph.\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 1.0837 - accuracy: 0.3833\n",
      "Epoch 2/15\n",
      "120/120 [==============================] - 0s 133us/step - loss: 1.0368 - accuracy: 0.4583\n",
      "Epoch 3/15\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.9920 - accuracy: 0.5583\n",
      "Epoch 4/15\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.9505 - accuracy: 0.6000\n",
      "Epoch 5/15\n",
      "120/120 [==============================] - 0s 175us/step - loss: 0.9125 - accuracy: 0.6583\n",
      "Epoch 6/15\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.8764 - accuracy: 0.7000\n",
      "Epoch 7/15\n",
      "120/120 [==============================] - 0s 183us/step - loss: 0.8423 - accuracy: 0.7500\n",
      "Epoch 8/15\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.8100 - accuracy: 0.7750\n",
      "Epoch 9/15\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.7796 - accuracy: 0.8000\n",
      "Epoch 10/15\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.7507 - accuracy: 0.8083\n",
      "Epoch 11/15\n",
      "120/120 [==============================] - 0s 175us/step - loss: 0.7227 - accuracy: 0.8083\n",
      "Epoch 12/15\n",
      "120/120 [==============================] - 0s 199us/step - loss: 0.6968 - accuracy: 0.8333\n",
      "Epoch 13/15\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.6721 - accuracy: 0.8500\n",
      "Epoch 14/15\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.6492 - accuracy: 0.8583\n",
      "Epoch 15/15\n",
      "120/120 [==============================] - 0s 191us/step - loss: 0.6278 - accuracy: 0.8583\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x284b4703748>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the neural network.\n",
    "model.fit(inputs_train_white, outputs_train, epochs=15, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Setosa', 'Virginica', 'Virginica', 'Virginica', 'Virginica',\n",
       "       'Setosa', 'Virginica', 'Setosa', 'Versicolor', 'Setosa',\n",
       "       'Virginica', 'Versicolor', 'Virginica', 'Versicolor', 'Virginica',\n",
       "       'Setosa', 'Virginica', 'Virginica', 'Virginica', 'Versicolor',\n",
       "       'Versicolor', 'Setosa', 'Versicolor', 'Virginica', 'Setosa',\n",
       "       'Setosa', 'Virginica', 'Setosa', 'Setosa', 'Virginica'],\n",
       "      dtype='<U10')"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Have the network predict the classes of the test inputs.\n",
    "predictions = model.predict(pca.transform(inputs_test))\n",
    "predictions_labels = encoder.inverse_transform(predictions)\n",
    "predictions_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(predictions_labels == encoder.inverse_transform(outputs_test)).sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
